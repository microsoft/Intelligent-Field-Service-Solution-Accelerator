{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getSecret(secretName):\n",
        "    linked_service = \"ifmpmvault\"\n",
        "    akv_name = \"ifm-vault\"\n",
        "\n",
        "    # Fetch the key from Azure Key Vault\n",
        "    secretValue = mssparkutils.credentials.getSecret(\n",
        "        linkedService=linked_service,\n",
        "        akvName=akv_name, \n",
        "        secret=secretName)\n",
        "    return secretValue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Help link to learn about AD authentication for API calls \n",
        "#https://learn.microsoft.com/en-us/rest/api/servicebus/get-azure-active-directory-token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
        "from azureml.core.model import Model\n",
        "from azureml.core import Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.webservice import Webservice\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from azureml.core.model import Model\n",
        "\n",
        "subscription_id = getSecret('subscription-id')\n",
        "resource_group = getSecret('resource-group')\n",
        "workspace_name = getSecret('workspace-name')\n",
        "\n",
        "ws = Workspace(workspace_name = workspace_name,\n",
        "               subscription_id = subscription_id,\n",
        "               resource_group = resource_group)\n",
        "\n",
        "# pull all metrics of best run\n",
        "experiment_name = 'PredictiveMaintenanceExperiment'\n",
        "\n",
        "for automl_run in ws.experiments[experiment_name].get_runs():\n",
        "    best_run, fitted_model = automl_run.get_output()  # We are taking the first run. You can update this if you like to take a different run\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "feat_data = spark.sql('''select * from machine_data_features''')\n",
        "\n",
        "split_date = \"2022-12-21\"\n",
        "training = feat_data.filter(feat_data.dt_truncated < split_date)\n",
        "testing = feat_data.filter(feat_data.dt_truncated >= split_date)\n",
        "\n",
        "X_test = testing.toPandas()\n",
        "drop_cols =['msdyn_customerassetid','dt_truncated','msdyn_name', 'msdyn_productname', 'modifiedon','component', 'failure','label_e']\n",
        "# Remove the extra names if that are in the input_features list\n",
        "input_features = [x for x in feat_data.columns if x not in set(drop_cols)]\n",
        "pd_predictions = pd.DataFrame(fitted_model.predict(X_test[input_features]), columns=['prediction'])\n",
        "\n",
        "X_test['prediction'] = pd_predictions['prediction'].tolist()\n",
        "\n",
        "df_prediction = spark.createDataFrame(X_test)\n",
        "df_prediction.write.mode(\"overwrite\").saveAsTable(\"machine_failure_prediction_score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pip install azure-identity azure-servicebus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "from pyspark.sql.functions import * \n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "sql_stmt = '''select distinct p.msdyn_customerassetid,msdyn_name,prediction,score from\n",
        "(\n",
        "    (select distinct msdyn_customerassetid,prediction, 0.95 as score from  machine_failure_prediction_score where prediction != 0 limit 2) as p \n",
        "    inner join dataverse_d365env_generated_data.msdyn_customerasset as m on p.msdyn_customerassetid = m.msdyn_customerassetid\n",
        ")''' #adjust this code to get predict_proba for score later\n",
        "df_predictions = spark.sql(sql_stmt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from azure.identity import DefaultAzureCredential,ManagedIdentityCredential\n",
        "from azure.servicebus import ServiceBusClient, ServiceBusMessage\n",
        "\n",
        "\n",
        "servicebus_queue_name = \"sbmessage\"\n",
        "\n",
        "# Create a ServiceBusClient using the credential and namespace\n",
        "servicebus_client = ServiceBusClient.from_connection_string(conn_str=getSecret('service-bus-connection-string'), logging_enable=True)\n",
        "\n",
        "# Create a ServiceBusSender using the client and queue name\n",
        "servicebus_sender = servicebus_client.get_queue_sender(servicebus_queue_name)\n",
        "\n",
        "df = df_predictions.collect()\n",
        "\n",
        "servicedate = str((datetime.now() + timedelta(days=0, hours=36)))\n",
        "\n",
        "for row in df:\n",
        "    pm_data = {\n",
        "        \"deviceid\": \"SimulatedThermostat-XSOL-WF3\", #row.msdyn_name, \n",
        "        \"eventtoken\": str(uuid.uuid4()),\n",
        "        \"reading\": str(row.score),\n",
        "        \"readingtype\": \"PredictiveMaintenance\",\n",
        "        \"ruleoutput\": \"AlarmPredMaint\",\n",
        "        \"time\": str(datetime.now()),\n",
        "        \"servicebydate\": servicedate,\n",
        "        \"score\": str(row.score)\n",
        "    }\n",
        "\n",
        "    #response = requests.post(queueUrl, headers=headers, json=pm_data)\n",
        "    pm_data_string = json.dumps(pm_data)\n",
        "    message = ServiceBusMessage(pm_data_string)\n",
        "\n",
        "    message = ServiceBusMessage(\n",
        "       pm_data_string,\n",
        "       content_type='application/json'\n",
        "   )\n",
        "\n",
        "    with servicebus_sender:\n",
        "        response = servicebus_sender.send_messages(message)\n",
        "        print(response)\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
